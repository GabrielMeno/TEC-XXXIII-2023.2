{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução as *RNNs* biblioteca Keras\n",
        "\n",
        "Neste notebook iremos praticar a sintaxe básica para treinar uma rede neural artificial recorrente utilizando as bibliotecas [Keras](https://keras.io/) e [Tensorflow](https://www.tensorflow.org/).\n",
        "\n",
        "<br/><br/>\n",
        "Embora nós iremos importar diretamente apenas objetos e funções da biblioteca Keras, a biblioteca tensorflow é necessária para que o treinamento das redes neurais seja feito."
      ],
      "metadata": {
        "id": "U5GgcYY8c9Wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import random as python_random\n",
        "\n",
        "np.random.seed(42)\n",
        "python_random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "tf.config.list_physical_devices()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ZvgCNzWpK3Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A biblioteca Keras possui diversos datasets pré-processados para facilitar a execução de testes. Além disso, essa disponibilização facilita o aprendizado de sua utilização.\n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "Abaixo iremos importar o dataset IMDB. Este é um dataset utilizado para *classificação de sentimento* em reviews sobre filmes. Esses reviews podem ser classificados como positivos ou negativos. Este dataset é também um clássico utilizado para testes de RNNs."
      ],
      "metadata": {
        "id": "9E2r1WIBc9Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, train_info = tfds.load('imdb_reviews', split='train', with_info=True, batch_size=-1)\n",
        "test, test_info = tfds.load('imdb_reviews', split='test', with_info=True, batch_size=-1)\n",
        "\n",
        "x_train, y_train = train[\"text\"], train[\"label\"]\n",
        "x_test, y_test = test[\"text\"], test[\"label\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VYjCiUXoLiUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando lidamos com dados sequenciais, devemos padronizar o tamanho das sequências para que todas tenham o mesmo número de símbolos. Isto se deve pelo fato das bibliotecas realizarem otimizações internas para que a execução do código seja mais veloz.\n",
        "\n",
        "<br />\n",
        "\n",
        "A forma correta de se fazer é selecionar um tamanho padrão para as sequências e, então, truncar as sequências maiores do que o limite ou adicionar um caractere *nulo* para preencher as sequências menores que o limite.\n",
        "\n",
        "\n",
        "<br />\n",
        "\n",
        "Pensando em facilitar a vida dos praticantes de *machine learning*, a biblioteca Keras nos fornece uma função auxiliar que permite fazer esta operação com poucas linhas de código. Da mesma forma, outras operações comuns ao processamento de textos também são implementadas no módulo `sequence` da biblioteca.\n",
        "\n",
        "Abaixo, criamos um `layer` customizado que irá adptar as entradas em formato de texto para que possamos inserir na rede neural durante o treinamento. Note que a função `preprocess` também realiza a *limpeza* do texto, i.e., remove caracteres indesejados, como tags `HTML`. No caso do dataset já ter sido pré-processado e e estar \"limpo\", podemos apenas realizar o retorno do texto puro ou então tornar todas as letras minúsculas.\n",
        "\n",
        "**Observação**: Decidir sobre o pré-processamento, trocar do *case* das letras, etc, faz parte do treino da rede neural recorrente para processamento de linguagem!\n",
        "\n",
        "A classe `TextVectorization` irá criar `layer` customizado, propriamente dito. Devemos definir a quantidade máxima de palavras considerada no dicionário da rede neural (`max_tokens`) e o tamanho máximo de cada texto a ser tratado pela rede neural (`output_sequence_length`). Perceba também que o nosso pré-processamento é vinculado ao `layer` em `standardize`.\n",
        "\n",
        "```\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None,\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    split='whitespace',\n",
        "    ngrams=None,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=None,\n",
        "    pad_to_max_tokens=False,\n",
        "    vocabulary=None,\n",
        "    idf_weights=None,\n",
        "    sparse=False,\n",
        "    ragged=False,\n",
        "    encoding='utf-8',\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ReLwzijuc9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess(texto):\n",
        "    lowercase = tf.strings.lower(texto)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "text_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=30000,\n",
        "    standardize=preprocess,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=80)\n",
        "text_vectorizer.adapt(x_train)"
      ],
      "metadata": {
        "id": "ONb532Jrganv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O formato do dataset IMBD traz apenas a informação de que o *target* é positivo (1) ou negativo (0). Por isso, devemos modificar o dataset para que incluir tantas colunas quanto necessário de forma que cada *target* possua `N` colunas (uma para cada tipo de alvo), todas preenchidas com o valor 0. Em seguida, invertemos o valor da coluna correspondente ao *target* correto.\n",
        "\n",
        "<br />\n",
        "\n",
        "Esta operação se chama *One-hot encoding*. Note que esta operação não foi necessária para os exercícios sobre o *dataset* MNIST já que o mesmo nos foi fornecido pré-processado."
      ],
      "metadata": {
        "id": "KviLEjs2c9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "rdXt9higNafr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos utilizar mais uma vez o modelo básico da biblioteca Keras que é o modelo `Sequential`. Este modelo nos permite adicionar camadas na rede neural que serão processadas na sequência em que forem inseridas. Ao criarmos um objeto deste tipo, podemos adicionar as camadas utilizando o método `add` do modelo.\n",
        "\n",
        "<br/>\n",
        "\n",
        "A primeira camada de qualquer modelo que faça processamento de textos é incluir uma camada chamada `Embedding`.\n",
        "\n",
        "```\n",
        "tf.keras.layers.Embedding(\n",
        "    input_dim,\n",
        "    output_dim,\n",
        "    embeddings_initializer='uniform',\n",
        "    embeddings_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    embeddings_constraint=None,\n",
        "    mask_zero=False,\n",
        "    input_length=None,\n",
        "    sparse=False\n",
        ")\n",
        "```\n",
        "\n",
        "Esta camada irá codificar uma representação para as palavras que estão presentes no dataset. Para adicionar esta camada, devemos indicar a quantia de palavras que queremos incluir (também chamado de tamanho do vocabulário), e o tamanho desta representação (isto é, o número de dimenções para estas representações). Por exemplo:\n",
        "\n",
        "```\n",
        "tf.keras.layers.Embedding(1000, 32)\n",
        "```\n",
        "\n",
        "<br/>\n",
        "\n",
        "Vimos também na aula de hoje as camadas recorrentes, que na biblioteca Keras são chamadas `SimpleRNN` e `LSTM` pela biblioteca Keras. Para cada camada, devemos informar o tamanho (número de unidades), e os valores de *dropout* e *recurrent_dropout* para a camada. De maneira geral, o *recurrent_dropout* é utilizado em casos em que a rede neural é muito grande, na casa de bilhões de parâmetros.\n",
        "\n",
        "```\n",
        "tf.keras.layers.SimpleRNN(\n",
        "    units,\n",
        "    activation='tanh',\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    recurrent_initializer='orthogonal',\n",
        "    bias_initializer='zeros',\n",
        "    kernel_regularizer=None,\n",
        "    recurrent_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    recurrent_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    dropout=0.0,\n",
        "    recurrent_dropout=0.0,\n",
        "    return_sequences=False,\n",
        "    return_state=False,\n",
        "    go_backwards=False,\n",
        "    stateful=False,\n",
        "    unroll=False,\n",
        "    **kwargs\n",
        ")\n",
        "\n",
        "tf.keras.layers.LSTM(\n",
        "    units,\n",
        "    activation='tanh',\n",
        "    recurrent_activation='sigmoid',\n",
        "    use_bias=True,\n",
        "    kernel_initializer='glorot_uniform',\n",
        "    recurrent_initializer='orthogonal',\n",
        "    bias_initializer='zeros',\n",
        "    unit_forget_bias=True,\n",
        "    kernel_regularizer=None,\n",
        "    recurrent_regularizer=None,\n",
        "    bias_regularizer=None,\n",
        "    activity_regularizer=None,\n",
        "    kernel_constraint=None,\n",
        "    recurrent_constraint=None,\n",
        "    bias_constraint=None,\n",
        "    dropout=0.0,\n",
        "    recurrent_dropout=0.0,\n",
        "    return_sequences=False,\n",
        "    return_state=False,\n",
        "    go_backwards=False,\n",
        "    stateful=False,\n",
        "    time_major=False,\n",
        "    unroll=False\n",
        ")\n",
        "```\n",
        "\n",
        "<br />\n",
        "\n",
        "Um detalhe de implementação: caso queiramos utilizar mais do que uma camada recorrente, precisamos setar `return_sequences=True` em todas as camadas recorrentes com exceção da última.\n",
        "\n",
        "*Observação*: Ainda há a camadda `GRU`, que é uma simplificação da `LSTM`.\n",
        "\n",
        "<br />\n",
        "\n",
        "Outro detalhe que deve ser observado sobre as redes neurais recorrentes é o fato de que a inicialização de seus parâmetros difere das redes neurais *feedforward* (isto é, as redes neurais comuns) e das *Conv Nets*. De mandeira geral, inicializamos os parâmetros das RNNs selecionando-os em uma distribuição uniforme, onde todos os valores tem a mesma chance de ser selecionados, utilizando um intervalo reduzido (exemplos: $[-0.1, 0.1], [-0.05, 0.05]$, etc). Para isso, devemos importar o tipo de inicialização correta e passar a função como um parâmetro quando formos criar as camadas. A inicialização uniforme na biblioteca Keras é chamada de `RandomUniform`."
      ],
      "metadata": {
        "id": "Jv0GJLIac9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "print('Build model...')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "model.add(text_vectorizer)\n",
        "\n",
        "model.add(tf.keras.layers.Embedding(30001, 32))\n",
        "\n",
        "model.add(tf.keras.layers.SimpleRNN(64,\n",
        "                                    use_bias=True,\n",
        "                                    return_sequences=False, #altere para True quando tiver mais do que uma camada recorrente\n",
        "                                    kernel_initializer= tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=42),\n",
        "                                    recurrent_dropout=0.0,\n",
        "                                    bias_initializer='zeros'))\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Gs1T_HQLLAqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de treinarmos o modelo, precisamos fazer sua compilação com o método `compile`. Isto utilizará a biblioteca tensorflow que fará diversas otimizações no código e irá gerar um executável na linguagem `C` (que não está imediatamente disponível para nós).\n",
        "\n",
        "<br/>\n",
        "\n",
        "Para compilar o código, precisamos passar qual a função de perda (`loss`), qual otimizador será utilizado para o treinamento (`optimizer`) e qual métrica será utilizada durante o treinamento para monitorarmos o progresso.\n",
        "\n",
        "<br/>\n",
        "\n",
        "A biblioteca keras oferece diversos otimizadores para treinamento. Para escolhermos, basta importar o otimizador desejado e fornecer para o método `compile`."
      ],
      "metadata": {
        "id": "tjnLp9eac9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop, Adadelta, Adam, Adagrad\n",
        "\n",
        "# aqui utilizaremos a função binary_crossentropy pois exatamente 2 classes e\n",
        "# otimizador Adam enquanto monitoramos a evolução da acurácia para o modelo\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "MJsj9G60LCfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set()\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18, 4))\n",
        "\n",
        "# loss\n",
        "ax1.plot(history.history['loss'])\n",
        "ax1.plot(history.history['val_loss'])\n",
        "ax1.set_title('Loss')\n",
        "ax1.set(xlabel='epoch', ylabel='loss')\n",
        "ax1.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "# acurácia\n",
        "ax2.plot(history.history['accuracy'])\n",
        "ax2.plot(history.history['val_accuracy'])\n",
        "ax2.set_title('Acurácia')\n",
        "ax2.set(xlabel='epoch', ylabel='accuracy')\n",
        "ax2.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "suQMXhgJsagU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A biblioteca Keras possui uma interface compatível com a biblioteca [scikit-learn](https://scikit-learn.org/stable/index.html), que é padrão para praticantes de *machine learning*. Por isso, podemos utilizar objetos e funções desta biblioteca para calcular métricas mais avançadas, como * precision*, * recall*  e * f1-score*.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Note que a rede neural faz uma previsão do * score*  para cada classe possível para cada exemplo no dataset. Ou seja, para cada exemplo, a saída da rede neural será um *score* para cada classe possível. No nosso caso são 2 classes e por isso a saída da rede neural conterá 2 *scores* para cada exemplo.\n",
        "\n",
        "<br/>\n",
        "\n",
        "Em função desta saída em formato de scores, precisamos ober o score mais alto obtido para cada exemplo. Este * score*  irá nos dizer qual a classe foi prevista pela rede neural com um grau de confiança maior. Isso será obtido pela função `argmax` da biblioteca numpy"
      ],
      "metadata": {
        "id": "QsrOjlRPc9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "print('Acurácia: {:.4f}'.format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))\n",
        "\n",
        "print('\\n\\nDemais métricas (separadas por classe): ')\n",
        "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "5g6xB6lFc9Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "print('Acurácia: {:.4f}'.format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1))))\n",
        "\n",
        "print('\\n\\nDemais métricas (separadas por classe): ')\n",
        "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "eAUPirdnc9Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)))"
      ],
      "metadata": {
        "id": "f9jxVD98AOxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase1 = 'the movie was great.'\n",
        "\n",
        "pred = model.predict(tf.convert_to_tensor([f'{frase1}'], dtype=tf.string) )"
      ],
      "metadata": {
        "id": "wy-r1WyBS9_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insira uma frase de teste abaixo\n",
        "frase1 = 'it was a really awesome movie'\n",
        "\n",
        "pred = model.predict(tf.convert_to_tensor([frase1], dtype=tf.string) )\n",
        "print(f'Probabilidade negativo: {(pred[0][0] * 100):.4f}% \\nProbabilidade positivo: {(pred[0][1] * 100):.4f}%')"
      ],
      "metadata": {
        "id": "bMCbedUBVk5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.14.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}